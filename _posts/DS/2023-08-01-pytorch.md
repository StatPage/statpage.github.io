---
layout: single
title:  "Pytorch 정리 노트"
toc: true
toc_sticky: true
# categories: [DS]
# tags: []
published: true
---

> 아래의 자료는 [Doxygen](https://statpage.github.io/july_project_meeting/) 스타일을 몸에 익히기 위해서 블로그 포스팅이 아닌 주석 작성을 주로 하였다.

# Pytorch란?



## torch

```python
## torch.ones()
#
# @param size (int): a sequence of integers defining the shape of the output tensor.
# @param out (Tensor, optional): the output tensor
# @param dtype (torch.dtype, optional): the desired data type of returned tensor
# @layout (torch.layout, optional)
# @device (torch.device, optinal)
# @drequires_grad (bool, optional)
torch.ones(size, *, out=None, dtype=None, layout=torch.strided, device=None, requires=grad=False)


## torch.cumsum()
#
# @param input (_): the input tensor
# @param dim (int): the dimension to do the operation over
# @param dtype (torchdtype, optional): the desired data type of returned tensor
# @param out (Tensor, optional): the ouput tensor
torch.cumsum(input, dim, *, dtype=None, out=None)


## torch.cat()
#
# @param tensors (sequence of Tensors): any python sequence of tensors of the same type
# @param dim (int, optional): the dimension over which the tensors are concatenated. 0: row, 1: column
# @param out (Tensor, optional): the dimension over which the tensors are concatenated
# 
# Concatenates the given sequence of seq tensors in the given dimension. In PyTorch, dim=-1 refers to the last dimension of a tensor. For example, consider a tensor t of shape (2, 3, 4). The last dimension of this tensor is of size 4. If we apply an operation with dim=-1, it means that the operation is applied along the last dimension of the tensor. If we apply t.sum(dim=-1), it will sum the elements of the tensor along the last dimension, resulting in a tensor of shape (2, 3).

For instance, if we apply t.sum(dim=-1), it will sum the elements of the tensor along the last dimension, resulting in a tensor of shape (2, 3).
torch.cat(tensors, dim=0, *, out=None)

# Example
x = torch.randn(2, 3)
# tensor([[ 0.6580, -1.0969, -0.4614],
#         [-0.1034, -0.5790,  0.1497]])

torch.cat((x, x, x), 0)
# tensor([[ 0.6580, -1.0969, -0.4614],
#         [-0.1034, -0.5790,  0.1497],
#         [ 0.6580, -1.0969, -0.4614],
#         [-0.1034, -0.5790,  0.1497],
#         [ 0.6580, -1.0969, -0.4614],
#         [-0.1034, -0.5790,  0.1497]])

torch.cat((x, x, x), 1)
# tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,
#          -1.0969, -0.4614],
#         [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,
#          -0.5790,  0.1497]])


## torch.tanh()
#
# @param input (Tensor): the input tensor
#
# Returns a new tensor with the hyperbolic tangent of the elements of input
torch.tanh(input, *, out=None)

# Example
x = torch.randn(4)
# tensor([ 0.8986, -0.7279,  1.1745,  0.2611])

torch.tanh(x)
# tensor([ 0.7156, -0.6218,  0.8257,  0.2553])


## torch.isnan()
#
# Checks if there are any NaN values in the tensor.


## torch.softmax()
#
# @param input (Tensor): input tensor
# @param dim (int): A dimension along which softmax will be computed
# @param dtype (torch.dtype, optional): the desired data type of returned tensor
#
# Softmax function to an n-dimensional input Tensor
torch.softmax(input, dim=None, dtype=None)

## torch.autograd.Variable()
#
# @param data (Tensor): Tensor to wrap
# @param requires_grad (bool, optional): If autograd should record operations on the returned variable.
#
# Wraps a tensor and records the operations applied to it. It also holds the gradient w.r.t. the tensor. Each tensor represents a node in a computational graph. If x is a Tensor that has x.requires_grad=True then x.grad is another Tensor holding the gradient of x with respect to some scalar value.
from torch.autograd import Variable

x = Variable(torch.ones(2, 2), requires_grad=True)
# tensor([[ 1.,  1.],
#         [ 1.,  1.]])

# epand_as method
# 
# @param other (Tensor): the result tensor has the same size as other
# 
# Returns a new view of the self tensor with singleton dimensions expanded to a larger size. Passing -1 as the size for a dimension means not changing the size of that dimension.
torch.tensor().expand_as()
torch.tensor().expand(sizes)

# Example
y = torch.tensor([[1], 
                  [2],
                  [3]])
y.expand_as(x)
# tensor([[ 1.,  1.],
#         [ 2.,  2.],
#         [ 3.,  3.]])

y.expand(-1, 4)   # -1 means not changing the size of that dimension. Same with y.expand(3, 4)
tensor([[ 1,  1,  1,  1],
        [ 2,  2,  2,  2],
        [ 3,  3,  3,  3]])


## torch.exp()
#
# @param input (Tensor): the input tensor
#
# Returns a new tensor with the exponential of the elements of the input tensor input
torch.exp(input, *, out=None)

# Example
torch.exp(torch.tensor([0, math.log(2.)]))
# tensor([ 1.,  2.])


## torch.zeros_like
#
# @param input (Tensor): the size of input will determine size of the output tensor
# @param dtype (torch.dtype, optional): the desired data type of returned tensor
# @param layout (torch.device, optional): the desired layout of returned tensor
# @param device (torch.device, optional): the desired device of returned tensor
# @param requires_grad(bool, optional): If autograd should record operations on the returned tensor
# @param memory_format (torch.memory_format, optional): the desired memory format of returned tensor
#
# Returns a tensor filled with the scalar value 0, with the same size as input
torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)

# Example
input = torch.empty(2, 3)
torch.zeros_like(input)
# tensor([[ 0.,  0.,  0.],
#        [ 0.,  0.,  0.]])
```

Reference: 
- [PyTorch](https://pytorch.org/)
- [신불사 - 신현호라 불리는 사나이](https://onecellboy.tistory.com/342)
- [Learning to Learn](https://badlec.tistory.com/287)

## torch.nn

### nn.BatchNorm1d
Pytorch를 쓰는 코드를 보면 보면 중간에 `nn.BatchNor1d(args.nw)`라는 Batch Normalization을 활용하는 코드가 보인다.

Batch Normalization을 사용하는 이유는 뭘까?

각 레이어마다 배치 데이터의 평균과 분산을 이용해서 정규화하는 방법.

하지만 이는 학습할 때만 사용한다. 학습할 때의 미니 배치 데이터의 평균과 분산을 이용해 정규화를 하고 학습을 잘 하자는 것이지 추론할 때 추론해야 하는 데이터의 평균과 분산을 통해서 다시 정규화를 하는 것은 학습한 데이터를 무의미하게 만드는 것이기 때문이다.

그래서 추론에는 학습에서 얻은 고정된 평균과 분산을 이용하고 모델의 `train` 모드와 `test` 모드를 구별하는 것이라고 한다.

### nn.Dropout

torch.nn.linear class는 linear transformation을 수행하는 class이다.

$$ y = xA^T + b $$

- weight $A$: the learnable weights of the module of shape (`out_features`, `in_features`). The value of this parameter is initialized **randomly** from $U(-\sqrt{k}, \sqrt{k})$, where $k = \frac{1}{in\_features}$
- bias $b$: the learnable bias of the module of shape (`out_features`). If `bias` is `True`, the values are initialized from $U(-\sqrt{k}, \sqrt{k})$ where $k = \frac{1}{in\_features}$

($k$는 `in_features`의 역수이다. `in_features`는 input의 feature 수이다.)

Dropout은 학습할 때 랜덤하게 뉴런을 꺼서 학습하는 방법이다. 이는 과적합을 방지하는 방법이다.

```python
## torch.nn.Linear()
#
# @param in_features (int): input sample size
# @param out_features (int): output sample size
# @bias (bool): If set to False, then no additive bias
#
# PyTorch module that applies a linear transformation to the input tensor.
torch.nn.Linear(in_features, out_features, bias=True, device)

# Example
lin = nn.Linear(20, 30)
input = torch.randn(128, 20)
output = m(input)
# torch.Size([128, 30])


## torch.nn.Dropout()
#
# @param p (float, optional): probability of an element to be zeroed
# @param inplace (bool, optional): if set to True, will do this operation in-place. This means that the output will be saved to the input
#
# During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper Dropout: A Simple Way to Prevent Neural Networks from Overfitting by Srivastava, Hinton et al.
import torch.nn as nn
nn.Dropout(p=0.5, inplace=False)

# example
m = nn.Dropout(p=0.2)
input = torch.randn(20, 16)
output = m(input)
output.size()
# torch.Size([20, 16])
```

Reference
- [Enough is not enough](https://eehoeskrap.tistory.com/430)

## torch_scatter

`torch_scatter` 패키지는 `torch`를 사용하는 프로젝트에서 자주 사용되는 연산들을 구현한 패키지이다.

그 중 `scatter` 함수는 node 들의 연산을 구현할 때 자주 사용되는 함수이다. deep-learning에서 다음 layer로 넘어가는 연산을 떠올리면 된다.

```python
from torch_scatter import scatter


## scatter
#
# @param src (Tensor): the source tensor
# @param index (Tensor): the indices of elements to scatter
# @param dim (int): the axis along which to index
# @param out (Tensor, optional): the destination tensor
# @param dim_size (int, optional): automatically create output tensor with size dim_size in dim dimension
# @param reduce (str, optional): the reduce operation ('add', 'mean', 'min', 'max', 'mul')
#
# scatter function is used to scatter the src tensor to out tensor along dim dimension according to index.
scatter(src: Tensor, index: Tensor, dim: int = -1, out: Tensor, dim_size: int, reduce: str = 'sum')
torch_scatter.scatter(src: Tensor, index: Tensor, dim: int = -1, out: Tensor, dim_size: int, reduce: str = 'sum')

# Example
src = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])
idx = torch.tensor([[0, 0, 1, 3], [2, 0, 3, 1]])
out = scatter(src, idx, dim=1, reduce='sum')
# tensor([[ 3,  3,  0,  4],
#         [ 6,  8,  5,  7]])

idx2 = torch.tensor([[0,0,1,1],[1,1,0,0]])
out2 = scatter(src, idx2, dim=0, reduce='sum')
# tensor([[1, 2, 7, 8],
#         [5, 6, 3, 4]])
```

Reference
- [pytorch_scatter](https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html)

## torch.Tensor

```python
## torch.Tensor()
#
# @param data (array_like): Initial data for the tensor
# @param dtype (torchdtype, optional): the desired data type of returned tensor
# @param device (torchdevice, optional): the desired device of returned tensor
# @param requires_grad (bool, optional): If autograd should record operations on the returned tensor
#
# Multi-dimensional matrix containing elements of a single data type. torch.Tensor is an alias for the default tensor type (torch.FloatTensor)
torch.Tensor(data, dtype=None, device=None, requires_grad=False)

# Example
torch.Tensor(2, 3)
# tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],
#        [ 0.0000e+00,  0.0000e+00, -1.5846e+29]])

torch.tensor([[1., -1.], [1., -1.]])
# tensor([[ 1.0000, -1.0000],
#         [ 1.0000, -1.0000]])

torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))
# tensor([[ 1,  2,  3],
#         [ 4,  5,  6]])


## torch.tensor.normal_
#
# @param mean (float): the mean of the normal distribution
# @param std (float): the standard deviation of the normal distribution
#
# Fills the tensor with elements samples from the normal distribution parameterized by mean and std
torch.tensor.normal_(mean=0, std=1, *, generator=None)
```